## 面試模擬問答：MVC 架構重構經驗

### ❓ 問題
你提到有參與從傳統 .NET Framework 4.5.2 重構到 MVC 架構並升級至 4.8 的過程。  
我想了解，這段重構的主要挑戰是什麼？你在這當中扮演了什麼角色？

### 💬 回答
當時正值 MVC 架構逐漸成為主流，那時我還是公司內比較新的成員，負責的任務是將訂單查詢功能從原本的傳統寫法進行重構。

挑戰主要是在理解 MVC 的設計精神，將原本寫在一起的邏輯做適當分離。像是商業邏輯我會放在 Module（或 Service）裡處理，而跟使用者互動或參數判斷的部分則放在 Controller，逐步建立起較為清晰的職責分離，也讓功能更好測試與維護。


## 面試模擬問答：One Code Base 多站共用架構

### ❓ 問題
你有提到參與「One Code base」專案，讓兩個電商網站共用一套程式碼。  
這種多站共用架構在實作上會有不少挑戰，能不能談談你們是怎麼做到共用與彈性並存的？有哪些具體做法或技術設計？

### 💬 回答
我們的電商實際上是兩個獨立網站：東森與森森。在專案初期，我們先完成東森網站從傳統架構轉換到 MVC，接著要處理森森時，我們發現兩者除了資料庫與前端樣式不同外，其實後端邏輯幾乎一致。

因此我們採用 One Code base 的方式來提升維護效率，讓兩站共用 Module 層的商業邏輯。如果有需要做差異化處理的部分，會從 Controller 層開始分流，森森與東森各自有獨立的 Controller，並透過參數控制進一步的邏輯選擇。

至於資料庫連線，我們是透過設定檔的連線字串來切換，不需要修改程式碼，平時開發時只要調整 config 設定，就能切換對應的站台環境。


## 面試模擬問答：ELK Stack 導入與日誌集中化經驗

### ❓ 問題
你有提到從一開始進 Server 抓實體檔 Log，到後來導入 ELK Stack。  
能說說當初導入 ELK 的動機與實際帶來的改變嗎？在導入過程中你負責哪些部分？有遇到什麼技術挑戰嗎？

### 💬 回答
當時電商流量逐漸成長，Server 數量也越來越多，過去需要一台一台連線進去看實體 log 檔的方式，效率已經跟不上需求。我們希望能有一個統一的平台來集中查看與搜尋日誌，因此啟動了導入 ELK Stack 的專案。

我主要負責的是 Filebeat 與 Logstash 的設定與流程設計。首先，我們在每台 Server 上安裝 Filebeat，設定好要讀取的目錄與對應檔案類型，並加上不同的 tag。Logstash 端則會根據這些 tag 做不同的解析邏輯，並加上一些額外欄位與 routing 規則，送進 Elasticsearch 的指定 index。

搭配這次轉型，我們也同時將原本的 log 輸出調整為 NLog 輸出 JSON 格式，讓後續在 Logstash 處理時更容易解析。

最大的挑戰在於 ELK 本身的學習曲線，包括 Logstash 的 filter 設定、Elasticsearch 的資料 mapping 與效能考量，以及在 Kibana 中如何建立適合的視覺化與搜尋條件。雖然當時對整體還在摸索階段，但這次實作經驗也讓我對 log 架構與可觀測性建立了更深的理解。

## 面試模擬問答：.NET Core 容器化與 Linux 環境部署經驗

### ❓ 問題
你提到曾經把系統容器化，並部署到 Linux 測試環境。  
可否說明一下這段過程中容器化帶來的好處是什麼？你是怎麼將 .NET Core 專案打包成 Docker 容器的？  
以及在 Linux 環境中是如何進行日誌查詢與錯誤排查的？

### 💬 回答
隨著專案開始使用 .NET Core，因為不再依賴 IIS，我們嘗試將應用程式容器化來提升部署彈性與效率。

具體做法是撰寫 Dockerfile，將 `dotnet restore`、`dotnet build`、`dotnet publish` 等步驟整合在裡面。接著使用 `docker build` 指令將專案打包成映像檔（image），並推送到指定的 Container Registry。之後在 Linux 測試環境上，使用 `docker pull` 下載映像檔，並用 `docker run` 指令啟動容器，完成應用部署。

容器化的好處是版本更新時可以快速替換映像檔，不需重新設定環境，部署流程大幅簡化。

在 Linux 環境的錯誤排查上，測試階段主要是透過 `docker logs` 指令查看容器內的 console 輸出。後續我們也部署了 Filebeat，將容器的 log 傳送到 ELK Stack，方便集中監控與快速定位問題。

## 面試模擬問答：微服務與 DDD 架構設計經驗

### ❓ 問題
你有提到近期專案往微服務與 DDD 架構方向重構。  
可以簡單說明一下你是如何在這個架構下設計服務邊界（Bounded Context）嗎？  
在實作上，有遇到什麼挑戰或心得分享嗎？

### 💬 回答
轉換到微服務與 DDD 架構確實是一個龐大的工程，目前我們主要集中在會員相關功能的重構階段。

在設計 Bounded Context 時，我們團隊（包含前後端工程師）參考 Event Storming 方法，透過討論來劃分服務邊界。依照大家對功能的熟悉度做拆分，雖然不是完全理論上的「正統」做法，但能符合現有開發節奏與需求。

在實作過程中，我們發現 DDD 與傳統 MVC 有不少差異，尤其在概念上的區別比較大。DDD 強調將商業邏輯放在 Domain 層，明確劃分 Domain、Repository、Aggregate Root 等角色，這對習慣 MVC 直接把邏輯寫在 Controller 或 Service 的我來說，初期學習曲線比較陡峭。

不過透過這次重構，我漸漸理解並體會到 DDD 的好處，例如能讓系統更具可維護性與擴展性，也更容易支援未來的微服務拆分。


## 面試模擬問答：行銷活動系統設計 - 登記型活動

### ❓ 問題
你提到負責多種行銷活動機制開發（MGM、任務型、登記型、投票活動等），  
能分享其中一個活動系統的設計重點或技術挑戰嗎？

### 💬 回答
我先分享登記型活動的設計。  
這個活動主要是讓符合條件的會員能參與登記，條件通常是下單符合特定規則，符合資格後會員可以進行登記，後續依據登記名單判斷是否贈送獎品。

設計重點在於控制每日可登記名額，且每個會員只能登記一次，同時要根據訂單中品號、分類等規則判斷資格。

為了提升查詢效率，我們會先整理下單時需紀錄的訂單資料，寫入一個專門的查詢用表。會員進行登記時，系統會判斷該會員在指定區間內的訂單是否符合資格。

此外，商品頁面也會根據這些規則判斷該商品是否能參與登記活動。這整個流程考驗了網站訂單與會員資料的整合能力與查詢效率，是一個相當實務且挑戰性的功能設計。


## 面試模擬問答：直播購物系統開發 - SignalR 即時推播

### ❓ 問題  
你如何利用 SignalR 在直播購物系統實現即時下單通知？過程中遇過什麼技術挑戰？

### 💬 回答  
直播購物系統整體流程是：  
粉絲團用戶在直播留言後，留言會通知直播系統。直播系統會比對留言中是否包含特定下單關鍵字，若符合，就會推送購買連結給用戶，引導完成下單。

起初我們嘗試使用 Facebook 的 Server Sent Event 來即時接收留言通知，但因為控制端在 Facebook，無法完全掌握，常會有漏訊問題。

後來改為在直播系統的 client 端建立 SignalR 連線，並將連線資訊儲存在 Redis。當直播系統接收到 Facebook Webhook 通知留言後，會根據 Redis 查詢相對應的 client ID，再透過 SignalR 將留言推送到前端聊天室呈現，並進行關鍵字分析，將購買訊息推送給用戶。

最大的挑戰是初次使用 SignalR，對其功能還不夠熟悉，但這方案成功解決了漏訊問題，提升了系統的即時互動體驗。


## 面試模擬問答：第三方支付串接 - 串接挑戰與對應策略

### ❓ 問題  
在整合 Line Pay、悠遊付等第三方支付過程中，有遇到什麼技術挑戰嗎？你是怎麼處理的？

### 💬 回答  
整合外部支付服務時，最大的挑戰是在使用者完成付款流程後的狀態同步問題。

常見問題包括：
- 使用者進入第三方頁面後，中途離開或中斷付款流程，導致無回呼。
- 第三方的回呼機制有時不夠穩定，或回傳格式不一致。
- 網路異常導致付款結果未即時同步，進而發生訂單狀態不一致的對帳問題。

遇到這類情況時，往往需要深入比對我方系統的交易資料與對方的訂單記錄，並與第三方支付廠商聯繫確認當下交易狀態，才能進行後續的帳務處理流程。

解法：
- 重要流程中加上「主動查詢 API」，確保付款狀態的最終一致性。
- 對於未完成訂單設計容錯機制，例如定時輪詢狀態或主動提示用戶完成付款。
- 串接初期與第三方技術窗口保持良好溝通，確認所有 callback 邏輯與交易節點。

這些經驗讓我對交易流程的完整性、穩定性設計有更深的體會。


## 面試模擬問答：會員手機驗證流程重構 - 使用 DDD 的經驗分享

### ❓ 問題  
你提到有用 DDD 模式重構會員手機驗證流程，可以分享當時的設計與經驗嗎？

### 💬 回答  
這次主要是根據 DDD 的精神來設計整個驗證流程。

我們從核心邏輯出發，優先定義 Domain Layer 中的重要 interface 與行為，例如：
- 發送驗證碼
- 查詢驗證碼
- 驗證驗證碼有效性

再往外實作 Infrastructure 層與 Application Service，將具體邏輯包裝好供上層使用，讓 UseCase 僅需關心流程調度與轉換。

因為是第一次實作 DDD，初期在架構理解與層次劃分上花了不少時間，也開始體會到：
- 把商業邏輯抽離、集中在 Domain 中維護，讓邏輯更清晰、模組更容易測試與擴充。
- 實作雖然一開始慢，但後期在維護、測試、與他人協作上都更有效率。

這次經驗讓我更能理解 DDD 在大型系統開發時的價值。


## 面試模擬問答：RabbitMQ 在電商導購分潤中的應用

### ❓ 問題  
請分享你在使用 RabbitMQ 的實務經驗，包含應用場景與架構設計挑戰。

### 💬 回答  
RabbitMQ 主要應用在電商網站的導購分潤與訂單後續處理。

### 🧩 應用場景  
當會員從外部導購平台（如比價網）點擊進入電商網站並完成訂單後，我們會：
1. 在 **成單流程中發送一筆訊息到 MQ**，包含會員來源與訂單資訊。
2. 後續 **由 Consumer 根據規則判斷是否符合分潤條件**，再發送資料到第三方平台進行結算。

這樣的設計實現了「**導購處理流程與主站訂單流程的解耦**」，避免主流程卡住或延遲。

此外，像是訂單通知信件等其他後續流程，也有透過 RabbitMQ 處理，但因為這部分非我主導，就不細談。

### ⚙️ 技術挑戰與架構說明  
- **Producer 設計**：  
  使用 API Gateway 作為消息入口，將訊息發送到指定的 `Exchange`。需要研究如何配置 Exchange、Routing Key 與 Queue 的綁定關係。
  
- **Consumer 實作**：  
  為了實現後台持續處理，採用 Windows Service 作為背景常駐服務進行消息消化與業務處理。這是較少接觸的開發模式，過程中也熟悉了相關的部署與監控技巧。

- **RabbitMQ 架構補充**：
  - 使用 **Direct Exchange** 做精準路由。
  - 消費端設有 **重試與錯誤佇列（Dead Letter Queue, DLQ）**，避免消息遺失。
  - 生產端配置 **持久化機制（durable queue, persistent message）**，保證在 broker 重啟後消息不遺失。
  - 所有佇列都有命名規則與 log trace ID 以利日後排查問題。

這個架構在實務上表現穩定，也幫助我們更好地分離責任、提高系統可擴充性。


## 面試模擬問答：接手陌生、結構混亂的專案會怎麼處理？

### ❓ 問題  
當你接手一個結構混亂、註解不足、測試也不完整的專案，你會怎麼開始理解與改善它？

### 💬 回答  

過去我會比較偏向「手動解析」，從源頭開始閱讀流程：
- 挑出每個 function 所做的事情，逐步釐清邏輯與資料流程。
- 找出可抽共用、複雜或重複的邏輯，開始做小規模的重構。
- 搭配實測與斷點進行理解與驗證。

但現在我會多一個新選項：**善用 AI 工具輔助理解與註解補全**。
- 初期使用 AI 幫助掃過整個專案，先整理大致的架構與流程。
- 同時補上 log 與必要的 function 說明，減少摸索時間。
- 再結合原本的步驟做逐步拆解，效率明顯提升。

這樣的方式對於快速進入陌生專案，或是接手 legacy code 都很有幫助。


## 面試模擬問答：參與過最具挑戰性的行銷專案？

### ❓ 問題  
你參與過哪些與行銷活動有關的專案？其中哪一個讓你印象最深、或你認為最具挑戰性？為什麼？

### 💬 回答  

如果說挑戰性較高的專案，我會提到「直播電商留言即時互動推播」這個案子。

這個專案的複雜性在於兩點：

1. **依賴 Facebook 留言 Webhook**：  
   我們原本嘗試用 Facebook 提供的 SSE（Server-Sent Events）來即時監控留言，但穩定性不足，常漏訊。由於我們無法掌控 Facebook 的通知機制，問題難以追蹤與修復。

2. **即時訊息推播設計與 SignalR 整合**：  
   為了解決穩定性的問題，我們改為在直撥系統建立聊天室時，利用 SignalR 綁定連線資訊並儲存在 Redis。當 Facebook 有留言通知來時，系統就能即時查出連線 ID，並主動將留言訊息推播至前端。這同時也伴隨關鍵字比對與推播行為，算是第一次比較完整地接觸即時通訊技術。

---

除了直播案子，近期也有幾個印象深刻的挑戰：

- **購物車改版：多商品選擇結帳功能**  
  原本系統以整筆訂單結帳為主，這次需在同一張訂單內根據使用者勾選切出新的結帳資訊，涉及原始結構調整、價格計算與 UI 流程修改。

- **集團 Line Webhook 統一蒐集方案**  
  各子系統原本各自串接 Line Notify，我負責設計統一的接收架構，透過 nginx 的 `mirror` 模式或 haproxy 做 request 分流，在不影響原有系統的前提下收集 webhook 資料。這案子牽涉到網路層的設定，對我來說是第一次比較深入碰觸 nginx / haproxy 的應用場景。

- **微服務架構下的基礎建設學習**  
  隨著專案往微服務靠攏，我也接觸了 k8s 部署，熟悉 deployment、pod、service、route 等資源的運作；log 處理也從原本 NLog ➜ Fluentd 轉為 Fluent Bit，在容器環境中實現更輕量的收集與轉發。

這些經歷不只讓我熟悉不同層面的技術，更訓練了我在面對未知技術時的整合與解決問題能力。


## 面試模擬問答：如何導入新技術並降低風險？

### ❓ 問題  
當你要導入一個新技術（例如 Kafka、SignalR、Fluent Bit、Kubernetes 等），你通常怎麼學習、評估與實際導入？你如何降低導入過程的風險與潛在技術債？

### 💬 回答  

我導入新技術時會遵循「漸進式學習與驗證」的流程，具體來說：

1. **從可控的環境著手**  
   以 Kubernetes 為例，我不會一開始就上 K8s，而是先在 Linux 實體機或 VM 中操作 Docker、Container，熟悉服務運行、資源限制、volume 掛載、log 管理等細節，並逐步轉向 K8s 的對應概念（如 deployment、pod、configmap、daemonset）。

2. **搭配小型測試專案進行驗證**  
   我會用 demo 專案或 sandbox 環境搭建技術 PoC，像 Fluentd、Fluent Bit 的導入就是我先在單台 server 或單一 container 上驗證 log 收集、轉發邏輯後，才導入到 cluster，避免一次全推造成觀察與 debug 的困難。

3. **優先觀察核心功能與異常處理邏輯**  
   我會著重觀察新技術在「錯誤處理、資源耗用、擴展性」上的行為，尤其是涉及分散式架構與即時處理的系統，像 SignalR 就會搭配 Redis 做分散推播測試。

4. **配合 log 與監控機制進行觀察**  
   除了功能測試外，我也會建立 log 流（Fluent Bit → Logstash → ES）來記錄服務執行情況，協助未來查錯。

5. **導入到實際專案前保留可回滾策略**  
   不論是透過 feature flag、雙軌系統、或 Helm rollback 設定，都會確保發現問題時可以快速回退，將風險控制在合理範圍。

這樣的方式讓我可以對新技術先「建立信心」，再逐步推進，降低失敗風險也能減少技術債累積。
